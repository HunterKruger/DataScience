{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:14.395809Z",
     "start_time": "2020-05-03T14:58:11.947968Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Flatten,Dense,Input,Dropout,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:14.435592Z",
     "start_time": "2020-05-03T14:58:14.397447Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_origin = pd.read_csv('../data/featured_data/X_train.csv')\n",
    "y_train_origin = pd.read_csv('../data/featured_data/y_train.csv')\n",
    "\n",
    "y_train_no_log = np.expm1(y_train_origin)\n",
    "\n",
    "X_train_origin = X_train_origin.to_numpy()\n",
    "y_train_no_log = y_train_no_log.to_numpy()\n",
    "y_train_no_log = y_train_no_log.ravel()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_origin, y_train_no_log, test_size=0.2, random_state=1) \n",
    "input_dim=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:15.349937Z",
     "start_time": "2020-05-03T14:58:14.437362Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=input_dim, activation='elu',kernel_initializer='he_normal', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(32, activation='elu',kernel_initializer='he_normal', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:15.354921Z",
     "start_time": "2020-05-03T14:58:15.350931Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:49.820644Z",
     "start_time": "2020-05-03T14:58:15.355918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/300\n",
      "1168/1168 [==============================] - 1s 916us/sample - loss: 39291230909.3699 - val_loss: 38026355641.8630\n",
      "Epoch 2/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 39289219605.0411 - val_loss: 38023815672.9863\n",
      "Epoch 3/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 39284881211.6164 - val_loss: 38018193436.0548\n",
      "Epoch 4/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 39275647480.9863 - val_loss: 38006863226.7397\n",
      "Epoch 5/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 39257828702.6849 - val_loss: 37986426318.9041\n",
      "Epoch 6/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 39227213683.7260 - val_loss: 37952286215.0137\n",
      "Epoch 7/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 39178856756.6027 - val_loss: 37902332633.4247\n",
      "Epoch 8/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 39109437383.8904 - val_loss: 37830638213.2603\n",
      "Epoch 9/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 39014977984.8767 - val_loss: 37735397740.7123\n",
      "Epoch 10/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 38889047250.4110 - val_loss: 37616967792.2192\n",
      "Epoch 11/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 38731160646.1370 - val_loss: 37465455798.3562\n",
      "Epoch 12/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 38538596520.3288 - val_loss: 37281437948.4931\n",
      "Epoch 13/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 38307992477.8082 - val_loss: 37071979674.3014\n",
      "Epoch 14/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 38042192573.3699 - val_loss: 36821619080.7671\n",
      "Epoch 15/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 37731488136.7671 - val_loss: 36535675132.4931\n",
      "Epoch 16/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 37378543616.0000 - val_loss: 36214284512.4384\n",
      "Epoch 17/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 36979875615.5616 - val_loss: 35857945529.8630\n",
      "Epoch 18/300\n",
      "1168/1168 [==============================] - 0s 108us/sample - loss: 36538946882.6301 - val_loss: 35447876958.6849\n",
      "Epoch 19/300\n",
      "1168/1168 [==============================] - 0s 117us/sample - loss: 36045812329.2055 - val_loss: 35013708196.8219\n",
      "Epoch 20/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 35522505124.8219 - val_loss: 34527484044.2740\n",
      "Epoch 21/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 34938036224.0000 - val_loss: 34011836864.8767\n",
      "Epoch 22/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 34313100919.2329 - val_loss: 33452540128.4384\n",
      "Epoch 23/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 33644081208.1096 - val_loss: 32870020081.9726\n",
      "Epoch 24/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 32944964285.3699 - val_loss: 32241061719.6712\n",
      "Epoch 25/300\n",
      "1168/1168 [==============================] - 0s 110us/sample - loss: 32200913625.4247 - val_loss: 31587313692.0548\n",
      "Epoch 26/300\n",
      "1168/1168 [==============================] - 0s 117us/sample - loss: 31418841733.2603 - val_loss: 30907294930.4110\n",
      "Epoch 27/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 30617551324.9315 - val_loss: 30182046790.1370\n",
      "Epoch 28/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 29779733265.5342 - val_loss: 29449296685.5890\n",
      "Epoch 29/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 28922487765.9178 - val_loss: 28679745816.5479\n",
      "Epoch 30/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 28041069722.3014 - val_loss: 27904514805.4795\n",
      "Epoch 31/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 27144010457.4247 - val_loss: 27126767952.6575\n",
      "Epoch 32/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 26240877778.4110 - val_loss: 26326143032.1096\n",
      "Epoch 33/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 25330156137.2055 - val_loss: 25523065336.9863\n",
      "Epoch 34/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 24431427640.1096 - val_loss: 24705385457.9726\n",
      "Epoch 35/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 23516347153.5342 - val_loss: 23932940933.2603\n",
      "Epoch 36/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 22635924886.7945 - val_loss: 23142240760.9863\n",
      "Epoch 37/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 21752538097.9726 - val_loss: 22375734959.3425\n",
      "Epoch 38/300\n",
      "1168/1168 [==============================] - 0s 106us/sample - loss: 20897551654.5753 - val_loss: 21613177533.3699\n",
      "Epoch 39/300\n",
      "1168/1168 [==============================] - 0s 113us/sample - loss: 20063738725.6986 - val_loss: 20876515832.9863\n",
      "Epoch 40/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 19255862622.6849 - val_loss: 20146407648.4384\n",
      "Epoch 41/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 18470801183.5616 - val_loss: 19445431941.2603\n",
      "Epoch 42/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 17716862162.4110 - val_loss: 18774042581.9178\n",
      "Epoch 43/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 16998559856.2192 - val_loss: 18137717521.5342\n",
      "Epoch 44/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 16326019885.5890 - val_loss: 17515844790.3562\n",
      "Epoch 45/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 15683084624.6575 - val_loss: 16942122194.4110\n",
      "Epoch 46/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 15083038299.1781 - val_loss: 16391507154.4110\n",
      "Epoch 47/300\n",
      "1168/1168 [==============================] - 0s 112us/sample - loss: 14515511127.6712 - val_loss: 15872378206.6849\n",
      "Epoch 48/300\n",
      "1168/1168 [==============================] - 0s 108us/sample - loss: 13993625459.7260 - val_loss: 15386376079.7808\n",
      "Epoch 49/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 13504915848.7671 - val_loss: 14929362572.2740\n",
      "Epoch 50/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 13055130399.5616 - val_loss: 14515135852.7123\n",
      "Epoch 51/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 12641414929.5342 - val_loss: 14117177617.5342\n",
      "Epoch 52/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 12260244241.5342 - val_loss: 13751588611.5068\n",
      "Epoch 53/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 11907984706.6301 - val_loss: 13424704259.5068\n",
      "Epoch 54/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 11587278679.6712 - val_loss: 13101302173.8082\n",
      "Epoch 55/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 11285464821.4795 - val_loss: 12824880955.6164\n",
      "Epoch 56/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 11014795516.4932 - val_loss: 12545045812.6027\n",
      "Epoch 57/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 10767594103.2329 - val_loss: 12291961607.0137\n",
      "Epoch 58/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 10533946115.5068 - val_loss: 12069891229.8082\n",
      "Epoch 59/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 10325646602.5205 - val_loss: 11858257927.0137\n",
      "Epoch 60/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 10126444684.2740 - val_loss: 11663221973.9178\n",
      "Epoch 61/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 9946763320.1096 - val_loss: 11474186162.8493\n",
      "Epoch 62/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 9774849935.7808 - val_loss: 11306289036.2740\n",
      "Epoch 63/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 9614764424.7671 - val_loss: 11147362503.8904\n",
      "Epoch 64/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 9461014317.5890 - val_loss: 10991357503.1233\n",
      "Epoch 65/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 95us/sample - loss: 9312217592.9863 - val_loss: 10836703270.5753\n",
      "Epoch 66/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 9169994120.7671 - val_loss: 10692804039.8904\n",
      "Epoch 67/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 9031925626.7397 - val_loss: 10553538731.8356\n",
      "Epoch 68/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 8896909999.3425 - val_loss: 10422530009.4247\n",
      "Epoch 69/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 8767280313.8630 - val_loss: 10285888550.5753\n",
      "Epoch 70/300\n",
      "1168/1168 [==============================] - 0s 107us/sample - loss: 8637171305.2055 - val_loss: 10160347844.3836\n",
      "Epoch 71/300\n",
      "1168/1168 [==============================] - 0s 102us/sample - loss: 8511595477.9178 - val_loss: 10036489738.5205\n",
      "Epoch 72/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 8385547193.8630 - val_loss: 9910749194.5205\n",
      "Epoch 73/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 8260571318.3562 - val_loss: 9786592540.0548\n",
      "Epoch 74/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 8137948468.6027 - val_loss: 9663734927.7808\n",
      "Epoch 75/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 8016657302.7945 - val_loss: 9545543837.8082\n",
      "Epoch 76/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 7897759126.7945 - val_loss: 9426623772.0548\n",
      "Epoch 77/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 7776600463.7808 - val_loss: 9312648560.2192\n",
      "Epoch 78/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 7655974631.4521 - val_loss: 9196732735.1233\n",
      "Epoch 79/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 7536828282.7397 - val_loss: 9077236781.5890\n",
      "Epoch 80/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 7417537101.1507 - val_loss: 8958477960.7671\n",
      "Epoch 81/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 7295818359.2329 - val_loss: 8842512450.6301\n",
      "Epoch 82/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 7173869897.6438 - val_loss: 8728271777.3151\n",
      "Epoch 83/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 7053840917.0411 - val_loss: 8607498268.0548\n",
      "Epoch 84/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 6931220743.0137 - val_loss: 8490012885.9178\n",
      "Epoch 85/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 6810770719.5616 - val_loss: 8371222405.2603\n",
      "Epoch 86/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 6688355335.0137 - val_loss: 8254424614.5753\n",
      "Epoch 87/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 6566130828.2740 - val_loss: 8136176117.4795\n",
      "Epoch 88/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 6444317148.9315 - val_loss: 8008442620.4932\n",
      "Epoch 89/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 6320122389.0411 - val_loss: 7894208669.8082\n",
      "Epoch 90/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 6195575387.1781 - val_loss: 7770193053.8082\n",
      "Epoch 91/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 6070417548.2740 - val_loss: 7649566912.8767\n",
      "Epoch 92/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 5946168607.5616 - val_loss: 7523574093.1507\n",
      "Epoch 93/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 5821196386.1918 - val_loss: 7394889938.4110\n",
      "Epoch 94/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 5695018341.6986 - val_loss: 7264712167.4521\n",
      "Epoch 95/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 5570667835.6164 - val_loss: 7135189086.6849\n",
      "Epoch 96/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 5446759648.4384 - val_loss: 7002818300.4932\n",
      "Epoch 97/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 5320125734.5753 - val_loss: 6877115623.4521\n",
      "Epoch 98/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 5199379259.6164 - val_loss: 6747603761.0959\n",
      "Epoch 99/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 5075662833.9726 - val_loss: 6616751626.5205\n",
      "Epoch 100/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 4955873287.0137 - val_loss: 6478263643.1781\n",
      "Epoch 101/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 4833781602.1918 - val_loss: 6352641799.0137\n",
      "Epoch 102/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 4714750520.1096 - val_loss: 6215832284.9315\n",
      "Epoch 103/300\n",
      "1168/1168 [==============================] - 0s 100us/sample - loss: 4594858699.3973 - val_loss: 6081117425.9726\n",
      "Epoch 104/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 4476386395.1781 - val_loss: 5944583620.3836\n",
      "Epoch 105/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 4357719646.6849 - val_loss: 5818259855.7808\n",
      "Epoch 106/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 4239678681.4247 - val_loss: 5684047433.6438\n",
      "Epoch 107/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 4124351137.3151 - val_loss: 5548865841.0959\n",
      "Epoch 108/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 4008942890.0822 - val_loss: 5422685811.7260\n",
      "Epoch 109/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 3899061710.9041 - val_loss: 5295621740.7123\n",
      "Epoch 110/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 3787554083.0685 - val_loss: 5166222784.8767\n",
      "Epoch 111/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 3679733500.4932 - val_loss: 5043421247.1233\n",
      "Epoch 112/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 3572229456.6575 - val_loss: 4917010463.5616\n",
      "Epoch 113/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 3469257507.0685 - val_loss: 4792390414.0274\n",
      "Epoch 114/300\n",
      "1168/1168 [==============================] - 0s 105us/sample - loss: 3366676795.6164 - val_loss: 4668988819.2877\n",
      "Epoch 115/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 3267018271.5616 - val_loss: 4544613193.6438\n",
      "Epoch 116/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 3172134631.4521 - val_loss: 4432313596.4932\n",
      "Epoch 117/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 3078189732.8219 - val_loss: 4309890928.2192\n",
      "Epoch 118/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 2988854678.7945 - val_loss: 4198175370.5205\n",
      "Epoch 119/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 2902138273.3151 - val_loss: 4088822529.7534\n",
      "Epoch 120/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 2817042018.1918 - val_loss: 3978428412.4932\n",
      "Epoch 121/300\n",
      "1168/1168 [==============================] - 0s 108us/sample - loss: 2735861816.1096 - val_loss: 3873608854.7945\n",
      "Epoch 122/300\n",
      "1168/1168 [==============================] - 0s 100us/sample - loss: 2658733504.8767 - val_loss: 3771807281.0959\n",
      "Epoch 123/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 2581447879.8904 - val_loss: 3674392665.4247\n",
      "Epoch 124/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 2510942257.0959 - val_loss: 3574784319.1233\n",
      "Epoch 125/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 2441335636.1644 - val_loss: 3484909606.5753\n",
      "Epoch 126/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 2372935038.2466 - val_loss: 3389450220.7123\n",
      "Epoch 127/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 2311680049.0959 - val_loss: 3303549155.9452\n",
      "Epoch 128/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 2250552965.2603 - val_loss: 3223419967.1233\n",
      "Epoch 129/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 2192558509.5890 - val_loss: 3140826915.0685\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 91us/sample - loss: 2138848122.7397 - val_loss: 3067993163.3973\n",
      "Epoch 131/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 2088404897.3151 - val_loss: 2994775196.0548\n",
      "Epoch 132/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 2035727475.7260 - val_loss: 2923166744.5479\n",
      "Epoch 133/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1988446714.7397 - val_loss: 2858338482.8493\n",
      "Epoch 134/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 1945456396.2740 - val_loss: 2794590802.4110\n",
      "Epoch 135/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1901775924.6027 - val_loss: 2736318712.9863\n",
      "Epoch 136/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 1862143111.0137 - val_loss: 2686602580.1644\n",
      "Epoch 137/300\n",
      "1168/1168 [==============================] - 0s 101us/sample - loss: 1824304599.6712 - val_loss: 2630707683.0685\n",
      "Epoch 138/300\n",
      "1168/1168 [==============================] - 0s 105us/sample - loss: 1789489287.0137 - val_loss: 2581729342.2466\n",
      "Epoch 139/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1756043337.6438 - val_loss: 2534289799.8904\n",
      "Epoch 140/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1724126416.6575 - val_loss: 2487239335.4521\n",
      "Epoch 141/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1695742793.6438 - val_loss: 2447767553.7534\n",
      "Epoch 142/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1667235932.9315 - val_loss: 2412894129.0959\n",
      "Epoch 143/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1639499244.7123 - val_loss: 2375393622.7945\n",
      "Epoch 144/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 1612943338.9589 - val_loss: 2335666588.0548\n",
      "Epoch 145/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1588589953.7534 - val_loss: 2300397127.0137\n",
      "Epoch 146/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1564971121.0959 - val_loss: 2277132297.6438\n",
      "Epoch 147/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1542911429.2603 - val_loss: 2242250112.8767\n",
      "Epoch 148/300\n",
      "1168/1168 [==============================] - 0s 108us/sample - loss: 1522290036.6027 - val_loss: 2216555887.3425\n",
      "Epoch 149/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1502191426.6301 - val_loss: 2195424053.4795\n",
      "Epoch 150/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1482600726.7945 - val_loss: 2167650834.4110\n",
      "Epoch 151/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1464705597.3699 - val_loss: 2145673942.7945\n",
      "Epoch 152/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1446384391.8904 - val_loss: 2120308362.5205\n",
      "Epoch 153/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1429043955.7260 - val_loss: 2096072490.0822\n",
      "Epoch 154/300\n",
      "1168/1168 [==============================] - 0s 102us/sample - loss: 1413649979.6164 - val_loss: 2077693337.4247\n",
      "Epoch 155/300\n",
      "1168/1168 [==============================] - 0s 108us/sample - loss: 1396557247.1233 - val_loss: 2058854641.0959\n",
      "Epoch 156/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1381189600.4384 - val_loss: 2033242260.1644\n",
      "Epoch 157/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1366437731.0685 - val_loss: 2020260715.8356\n",
      "Epoch 158/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1352660534.3562 - val_loss: 2005011417.4247\n",
      "Epoch 159/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1338793945.4247 - val_loss: 1989415296.8767\n",
      "Epoch 160/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1325061485.5890 - val_loss: 1976111091.2877\n",
      "Epoch 161/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 1312112904.7671 - val_loss: 1961149181.3699\n",
      "Epoch 162/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1299999511.6712 - val_loss: 1945207882.0822\n",
      "Epoch 163/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1287855843.9452 - val_loss: 1932058744.1096\n",
      "Epoch 164/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1276188099.5068 - val_loss: 1917416026.7397\n",
      "Epoch 165/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 1264718478.0274 - val_loss: 1908895848.7671\n",
      "Epoch 166/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 1254830045.1507 - val_loss: 1902217766.5753\n",
      "Epoch 167/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1243520995.0685 - val_loss: 1884509646.4658\n",
      "Epoch 168/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 1234250384.6575 - val_loss: 1872306705.9726\n",
      "Epoch 169/300\n",
      "1168/1168 [==============================] - 0s 100us/sample - loss: 1223622727.8904 - val_loss: 1869419737.8630\n",
      "Epoch 170/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1214198458.7397 - val_loss: 1859684447.1233\n",
      "Epoch 171/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 1204792062.6849 - val_loss: 1840404092.4932\n",
      "Epoch 172/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 1195603576.5479 - val_loss: 1834636975.7808\n",
      "Epoch 173/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 1187422996.1644 - val_loss: 1822440381.8082\n",
      "Epoch 174/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1179322170.3014 - val_loss: 1821022373.2603\n",
      "Epoch 175/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1170620922.7397 - val_loss: 1809551140.3836\n",
      "Epoch 176/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1162097319.0137 - val_loss: 1802435846.5753\n",
      "Epoch 177/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1153905639.4521 - val_loss: 1795754873.8630\n",
      "Epoch 178/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1146992530.4110 - val_loss: 1792038022.1370\n",
      "Epoch 179/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 1138140702.2466 - val_loss: 1777548751.7808\n",
      "Epoch 180/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1131331407.3425 - val_loss: 1768251413.4795\n",
      "Epoch 181/300\n",
      "1168/1168 [==============================] - 0s 102us/sample - loss: 1124447468.7123 - val_loss: 1768102627.9452\n",
      "Epoch 182/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1117586958.0274 - val_loss: 1756060495.7808\n",
      "Epoch 183/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1111086453.0411 - val_loss: 1749852778.9589\n",
      "Epoch 184/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1103677715.2877 - val_loss: 1747574548.1644\n",
      "Epoch 185/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1097576407.6712 - val_loss: 1751969026.1918\n",
      "Epoch 186/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1091138521.8630 - val_loss: 1740678766.0274\n",
      "Epoch 187/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1084756909.5890 - val_loss: 1731921516.2740\n",
      "Epoch 188/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1079453016.5479 - val_loss: 1727195658.0822\n",
      "Epoch 189/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1073872176.6575 - val_loss: 1723771567.7808\n",
      "Epoch 190/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1067606088.3288 - val_loss: 1717268955.1781\n",
      "Epoch 191/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1062107960.5479 - val_loss: 1709390319.7808\n",
      "Epoch 192/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 1056592088.1096 - val_loss: 1703106893.1507\n",
      "Epoch 193/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1053151885.1507 - val_loss: 1706938060.2740\n",
      "Epoch 194/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1046072412.9315 - val_loss: 1699480652.7123\n",
      "Epoch 195/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1044402430.2466 - val_loss: 1686979495.0137\n",
      "Epoch 196/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 1036976348.0548 - val_loss: 1691931404.7123\n",
      "Epoch 197/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1032513342.2466 - val_loss: 1680711476.1644\n",
      "Epoch 198/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1026850206.6849 - val_loss: 1683246542.0274\n",
      "Epoch 199/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 1021674173.3699 - val_loss: 1676053491.2877\n",
      "Epoch 200/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1016191693.1507 - val_loss: 1674056565.9178\n",
      "Epoch 201/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 1013756122.3014 - val_loss: 1669534318.9041\n",
      "Epoch 202/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 1008727450.3014 - val_loss: 1660393086.6849\n",
      "Epoch 203/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 1003046982.5753 - val_loss: 1659100290.1918\n",
      "Epoch 204/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 999488833.7534 - val_loss: 1659467944.7671\n",
      "Epoch 205/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 994885363.7260 - val_loss: 1658962193.0959\n",
      "Epoch 206/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 990542468.3836 - val_loss: 1654323830.7945\n",
      "Epoch 207/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 987934594.6301 - val_loss: 1651807986.4110\n",
      "Epoch 208/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 982964098.1918 - val_loss: 1646155192.9863\n",
      "Epoch 209/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 980471452.9315 - val_loss: 1640132171.8356\n",
      "Epoch 210/300\n",
      "1168/1168 [==============================] - 0s 104us/sample - loss: 976730835.2877 - val_loss: 1635317717.9178\n",
      "Epoch 211/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 975283760.2192 - val_loss: 1638797528.5479\n",
      "Epoch 212/300\n",
      "1168/1168 [==============================] - 0s 100us/sample - loss: 967903920.2192 - val_loss: 1635226567.4521\n",
      "Epoch 213/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 968370437.6986 - val_loss: 1619045866.0822\n",
      "Epoch 214/300\n",
      "1168/1168 [==============================] - 0s 100us/sample - loss: 960814274.6301 - val_loss: 1623184316.0548\n",
      "Epoch 215/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 957880484.8219 - val_loss: 1618108593.0959\n",
      "Epoch 216/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 953939578.3014 - val_loss: 1615346669.1507\n",
      "Epoch 217/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 951241429.0411 - val_loss: 1616202036.6027\n",
      "Epoch 218/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 947334709.0411 - val_loss: 1609654643.7260\n",
      "Epoch 219/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 944225404.9315 - val_loss: 1606441555.2877\n",
      "Epoch 220/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 941278584.1096 - val_loss: 1606178064.6575\n",
      "Epoch 221/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 937846174.6849 - val_loss: 1602867606.3562\n",
      "Epoch 222/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 935355592.7671 - val_loss: 1602437797.9178\n",
      "Epoch 223/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 933151295.1233 - val_loss: 1599884615.8904\n",
      "Epoch 224/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 929315430.5753 - val_loss: 1593640377.8630\n",
      "Epoch 225/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 926343954.4110 - val_loss: 1595428492.7123\n",
      "Epoch 226/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 923618798.4658 - val_loss: 1590636019.2877\n",
      "Epoch 227/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 921694386.8493 - val_loss: 1589496382.4658\n",
      "Epoch 228/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 918634083.0685 - val_loss: 1584204836.3836\n",
      "Epoch 229/300\n",
      "1168/1168 [==============================] - 0s 98us/sample - loss: 915051649.3151 - val_loss: 1585776196.3836\n",
      "Epoch 230/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 913633656.9863 - val_loss: 1586858482.6301\n",
      "Epoch 231/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 910960369.0959 - val_loss: 1578146988.4932\n",
      "Epoch 232/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 906984224.4384 - val_loss: 1581926101.0411\n",
      "Epoch 233/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 904854495.5616 - val_loss: 1578360097.3151\n",
      "Epoch 234/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 902934220.9315 - val_loss: 1573857869.8082\n",
      "Epoch 235/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 898962374.1370 - val_loss: 1570229408.0000\n",
      "Epoch 236/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 898047662.0274 - val_loss: 1573012928.6575\n",
      "Epoch 237/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 893918974.6849 - val_loss: 1565514763.3973\n",
      "Epoch 238/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 892316957.8082 - val_loss: 1565110761.8630\n",
      "Epoch 239/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 891445474.6301 - val_loss: 1561491280.4384\n",
      "Epoch 240/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 888138329.4247 - val_loss: 1564485400.3288\n",
      "Epoch 241/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 885056602.3014 - val_loss: 1556666755.9452\n",
      "Epoch 242/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 883948089.4247 - val_loss: 1557036387.2877\n",
      "Epoch 243/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 880607360.8767 - val_loss: 1559154218.0822\n",
      "Epoch 244/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 878849425.5342 - val_loss: 1549361910.3562\n",
      "Epoch 245/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 879187937.7534 - val_loss: 1548214656.0000\n",
      "Epoch 246/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 874606002.8493 - val_loss: 1551168541.8082\n",
      "Epoch 247/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 872296174.0274 - val_loss: 1549872133.9178\n",
      "Epoch 248/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 869721007.3425 - val_loss: 1548083736.1096\n",
      "Epoch 249/300\n",
      "1168/1168 [==============================] - 0s 97us/sample - loss: 868114666.0822 - val_loss: 1544771689.4247\n",
      "Epoch 250/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 865626496.0000 - val_loss: 1541472142.4658\n",
      "Epoch 251/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 864488086.3562 - val_loss: 1538409886.2466\n",
      "Epoch 252/300\n",
      "1168/1168 [==============================] - 0s 95us/sample - loss: 861903395.9452 - val_loss: 1542159431.4521\n",
      "Epoch 253/300\n",
      "1168/1168 [==============================] - 0s 96us/sample - loss: 859533064.3288 - val_loss: 1536740281.4247\n",
      "Epoch 254/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 857519036.0548 - val_loss: 1538903071.1233\n",
      "Epoch 255/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 855042062.9041 - val_loss: 1537788046.9041\n",
      "Epoch 256/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 852874036.1644 - val_loss: 1531479680.2192\n",
      "Epoch 257/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 851761295.7808 - val_loss: 1529348045.3699\n",
      "Epoch 258/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 852441222.1370 - val_loss: 1528770036.3836\n",
      "Epoch 259/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 847233149.8082 - val_loss: 1527682306.4110\n",
      "Epoch 260/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 91us/sample - loss: 846233499.1781 - val_loss: 1531506376.9863\n",
      "Epoch 261/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 844458576.6575 - val_loss: 1527828601.6438\n",
      "Epoch 262/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 842894063.7808 - val_loss: 1523646468.3836\n",
      "Epoch 263/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 841216077.1507 - val_loss: 1521145522.6301\n",
      "Epoch 264/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 838975160.1096 - val_loss: 1519859656.5479\n",
      "Epoch 265/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 837033982.6849 - val_loss: 1521517887.3425\n",
      "Epoch 266/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 835142997.4795 - val_loss: 1519367338.5205\n",
      "Epoch 267/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 834059819.3973 - val_loss: 1514712735.1233\n",
      "Epoch 268/300\n",
      "1168/1168 [==============================] - 0s 89us/sample - loss: 833269045.4795 - val_loss: 1518195136.2192\n",
      "Epoch 269/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 830362907.1781 - val_loss: 1509677345.5342\n",
      "Epoch 270/300\n",
      "1168/1168 [==============================] - 0s 89us/sample - loss: 828880057.8630 - val_loss: 1513568911.1233\n",
      "Epoch 271/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 826998518.3562 - val_loss: 1513324053.6986\n",
      "Epoch 272/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 825486208.4384 - val_loss: 1507475017.8630\n",
      "Epoch 273/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 822922814.6849 - val_loss: 1506609493.2603\n",
      "Epoch 274/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 821826801.5342 - val_loss: 1503852614.3562\n",
      "Epoch 275/300\n",
      "1168/1168 [==============================] - 0s 99us/sample - loss: 822334307.0685 - val_loss: 1509160348.2740\n",
      "Epoch 276/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 818477784.9863 - val_loss: 1504401328.2192\n",
      "Epoch 277/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 816892480.4384 - val_loss: 1501242221.3699\n",
      "Epoch 278/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 816465822.6849 - val_loss: 1503358849.3151\n",
      "Epoch 279/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 814440892.0548 - val_loss: 1496097230.2466\n",
      "Epoch 280/300\n",
      "1168/1168 [==============================] - 0s 89us/sample - loss: 812319763.2877 - val_loss: 1496905021.8082\n",
      "Epoch 281/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 810789979.6164 - val_loss: 1500612543.7808\n",
      "Epoch 282/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 809014805.4795 - val_loss: 1497007082.7397\n",
      "Epoch 283/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 810299629.5890 - val_loss: 1494910977.5342\n",
      "Epoch 284/300\n",
      "1168/1168 [==============================] - 0s 94us/sample - loss: 806303731.7260 - val_loss: 1496433350.7945\n",
      "Epoch 285/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 805782865.0959 - val_loss: 1496875872.2192\n",
      "Epoch 286/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 803138556.0548 - val_loss: 1491120493.8082\n",
      "Epoch 287/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 801809757.3699 - val_loss: 1491441379.9452\n",
      "Epoch 288/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 800696691.7260 - val_loss: 1491467316.6027\n",
      "Epoch 289/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 799046118.1370 - val_loss: 1489679880.1096\n",
      "Epoch 290/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 797738688.0000 - val_loss: 1489019553.0959\n",
      "Epoch 291/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 799054943.5616 - val_loss: 1482318182.3562\n",
      "Epoch 292/300\n",
      "1168/1168 [==============================] - 0s 89us/sample - loss: 794682197.0411 - val_loss: 1487086309.0411\n",
      "Epoch 293/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 796391083.8356 - val_loss: 1482294175.7808\n",
      "Epoch 294/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 793068528.6575 - val_loss: 1493488182.3562\n",
      "Epoch 295/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 789715891.2877 - val_loss: 1476727063.4521\n",
      "Epoch 296/300\n",
      "1168/1168 [==============================] - 0s 90us/sample - loss: 788336794.7397 - val_loss: 1478868356.1644\n",
      "Epoch 297/300\n",
      "1168/1168 [==============================] - 0s 93us/sample - loss: 788433693.1507 - val_loss: 1478428620.2740\n",
      "Epoch 298/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 786086354.4110 - val_loss: 1476546491.8356\n",
      "Epoch 299/300\n",
      "1168/1168 [==============================] - 0s 91us/sample - loss: 784985720.7671 - val_loss: 1479199934.9041\n",
      "Epoch 300/300\n",
      "1168/1168 [==============================] - 0s 92us/sample - loss: 784400912.6575 - val_loss: 1473264389.4795\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:51.673689Z",
     "start_time": "2020-05-03T14:58:49.821648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks([x for x in range(epoch+1)])\n",
    "plt.legend(['Train','Val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:58:51.678675Z",
     "start_time": "2020-05-03T14:58:51.674728Z"
    }
   },
   "outputs": [],
   "source": [
    "##  NN is not suitable for regression task, hard for hyperparameter tuning and explanation\n",
    "##  val_loss: 1473264389.4795\n",
    "##  RMSE > 38000, worse than ML models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
